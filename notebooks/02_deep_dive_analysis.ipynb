{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive Analysis - Advanced Signal Transforms for Anomaly Detection\n",
    "\n",
    "**Objective**: Find discriminative features/transforms that can separate anomaly classes (6,7,8) from known classes (0-5).\n",
    "\n",
    "Based on EDA findings:\n",
    "- Anomalies resemble low-power classes (3,4,5) in simple features\n",
    "- Class 7 is hardest (mixed with 3,4,5 in t-SNE)\n",
    "- Simple features achieve only 22% recall\n",
    "- Different IQ constellations suggest different modulation schemes\n",
    "\n",
    "**Approaches to explore:**\n",
    "1. Higher-order statistics (cumulants) - crucial for modulation classification\n",
    "2. Cyclostationary features - radio signals have periodic statistics\n",
    "3. Wavelet transforms - multi-resolution analysis\n",
    "4. Phase-based features - instantaneous phase, unwrapped phase dynamics\n",
    "5. Entropy measures - spectral, sample, permutation entropy\n",
    "6. Constellation geometry - cluster analysis of IQ plane\n",
    "7. Bispectrum analysis - higher-order spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_utils import load_train_data, load_test_anomalies, create_binary_labels, filter_by_snr\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "KNOWN_COLOR = 'tab:blue'\n",
    "ANOMALY_COLOR = 'tab:red'\n",
    "CLASS_COLORS = plt.cm.tab10(np.arange(10))\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_signals, train_labels, train_snr = load_train_data()\n",
    "test_signals, test_labels, test_snr = load_test_anomalies()\n",
    "\n",
    "# Filter out SNR=0 from training (not present in test)\n",
    "train_signals_filt, train_labels_filt, train_snr_filt = filter_by_snr(\n",
    "    train_signals, train_labels, train_snr, [10, 20, 30]\n",
    ")\n",
    "\n",
    "# Binary labels for test\n",
    "test_binary = create_binary_labels(test_labels)\n",
    "\n",
    "print(f\"Training data (filtered): {train_signals_filt.shape[0]} samples\")\n",
    "print(f\"Test data: {test_signals.shape[0]} samples\")\n",
    "print(f\"Test anomalies: {test_binary.sum()} ({100*test_binary.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Higher-Order Statistics (Cumulants)\n",
    "\n",
    "Cumulants are powerful for modulation classification. The 2nd, 4th, and 6th order cumulants can distinguish different modulation types.\n",
    "\n",
    "Key cumulants:\n",
    "- C20: E[x²] - variance\n",
    "- C21: E[|x|²] - power\n",
    "- C40: E[x⁴] - 3E[x²]² (4th-order, relates to kurtosis)\n",
    "- C41: E[x²|x|²] - E[x²]E[|x|²]\n",
    "- C42: E[|x|⁴] - |E[x²]|² - 2E[|x|²]²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complex_signal(signal):\n",
    "    \"\"\"Convert IQ to complex signal.\"\"\"\n",
    "    return signal[:, 0] + 1j * signal[:, 1]\n",
    "\n",
    "def compute_cumulants(signal):\n",
    "    \"\"\"\n",
    "    Compute higher-order cumulants for modulation classification.\n",
    "    Following Swami & Sadler (2000) formulation.\n",
    "    \"\"\"\n",
    "    x = compute_complex_signal(signal)\n",
    "    n = len(x)\n",
    "    \n",
    "    # Normalize to unit power\n",
    "    x = x / (np.sqrt(np.mean(np.abs(x)**2)) + 1e-10)\n",
    "    \n",
    "    # Moments\n",
    "    M20 = np.mean(x**2)\n",
    "    M21 = np.mean(np.abs(x)**2)  # Should be ~1 after normalization\n",
    "    M40 = np.mean(x**4)\n",
    "    M41 = np.mean((x**2) * (np.abs(x)**2))\n",
    "    M42 = np.mean(np.abs(x)**4)\n",
    "    M60 = np.mean(x**6)\n",
    "    M63 = np.mean((np.abs(x)**2)**3)\n",
    "    \n",
    "    # Cumulants\n",
    "    C20 = M20\n",
    "    C21 = M21\n",
    "    C40 = M40 - 3 * M20**2\n",
    "    C41 = M41 - 3 * M20 * M21\n",
    "    C42 = M42 - np.abs(M20)**2 - 2 * M21**2\n",
    "    C60 = M60 - 15 * M20 * M40 + 30 * M20**3\n",
    "    C63 = M63 - 9 * M42 * M21 + 12 * M21**3\n",
    "    \n",
    "    return {\n",
    "        'C20_abs': np.abs(C20),\n",
    "        'C20_angle': np.angle(C20),\n",
    "        'C21': np.real(C21),\n",
    "        'C40_abs': np.abs(C40),\n",
    "        'C40_angle': np.angle(C40),\n",
    "        'C41_abs': np.abs(C41),\n",
    "        'C42': np.real(C42),\n",
    "        'C60_abs': np.abs(C60),\n",
    "        'C63': np.real(C63),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulants for all signals\n",
    "print(\"Computing cumulants for training data...\")\n",
    "train_cumulants = [compute_cumulants(s) for s in train_signals_filt]\n",
    "train_cum_df = pd.DataFrame(train_cumulants)\n",
    "train_cum_df['class'] = train_labels_filt\n",
    "train_cum_df['snr'] = train_snr_filt\n",
    "\n",
    "print(\"Computing cumulants for test data...\")\n",
    "test_cumulants = [compute_cumulants(s) for s in test_signals]\n",
    "test_cum_df = pd.DataFrame(test_cumulants)\n",
    "test_cum_df['class'] = test_labels\n",
    "test_cum_df['snr'] = test_snr\n",
    "test_cum_df['is_anomaly'] = test_binary\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulant distribution by class\n",
    "print(\"\\nMean Cumulants by Class (Test Data):\")\n",
    "print(\"=\"*80)\n",
    "cumulant_cols = ['C20_abs', 'C40_abs', 'C41_abs', 'C42', 'C60_abs', 'C63']\n",
    "display_df = test_cum_df.groupby('class')[cumulant_cols].mean().round(4)\n",
    "display_df['is_anomaly'] = ['No']*6 + ['YES']*3\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key cumulants: Known vs Anomaly\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for i, col in enumerate(cumulant_cols):\n",
    "    ax = axes.flatten()[i]\n",
    "    known = test_cum_df[test_cum_df['is_anomaly'] == 0][col]\n",
    "    anomaly = test_cum_df[test_cum_df['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known (0-5)', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly (6-8)', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{col} Distribution')\n",
    "\n",
    "plt.suptitle('Higher-Order Cumulants: Known vs Anomaly', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_cumulants_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D scatter: C40 vs C42 (classic for modulation classification)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "for class_idx in range(9):\n",
    "    mask = test_cum_df['class'] == class_idx\n",
    "    color = ANOMALY_COLOR if class_idx >= 6 else CLASS_COLORS[class_idx]\n",
    "    marker = 'x' if class_idx >= 6 else 'o'\n",
    "    label = f'ANOMALY {class_idx}' if class_idx >= 6 else f'Class {class_idx}'\n",
    "    ax.scatter(test_cum_df[mask]['C40_abs'], test_cum_df[mask]['C42'],\n",
    "               c=[color], label=label, alpha=0.5, marker=marker, s=20)\n",
    "ax.set_xlabel('|C40|')\n",
    "ax.set_ylabel('C42')\n",
    "ax.set_title('Cumulant Space: |C40| vs C42')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "ax = axes[1]\n",
    "for class_idx in range(9):\n",
    "    mask = test_cum_df['class'] == class_idx\n",
    "    color = ANOMALY_COLOR if class_idx >= 6 else CLASS_COLORS[class_idx]\n",
    "    marker = 'x' if class_idx >= 6 else 'o'\n",
    "    ax.scatter(test_cum_df[mask]['C20_abs'], test_cum_df[mask]['C60_abs'],\n",
    "               c=[color], alpha=0.5, marker=marker, s=20)\n",
    "ax.set_xlabel('|C20|')\n",
    "ax.set_ylabel('|C60|')\n",
    "ax.set_title('Cumulant Space: |C20| vs |C60|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_cumulants_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Phase-Based Features\n",
    "\n",
    "The instantaneous phase and its derivatives contain modulation information:\n",
    "- Phase variance\n",
    "- Phase jitter (derivative of phase)\n",
    "- Phase transitions (how phase changes between samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phase_features(signal):\n",
    "    \"\"\"Compute phase-based features.\"\"\"\n",
    "    x = compute_complex_signal(signal)\n",
    "    \n",
    "    # Instantaneous phase\n",
    "    phase = np.angle(x)\n",
    "    \n",
    "    # Unwrapped phase (continuous)\n",
    "    phase_unwrap = np.unwrap(phase)\n",
    "    \n",
    "    # Phase derivative (instantaneous frequency)\n",
    "    phase_diff = np.diff(phase_unwrap)\n",
    "    \n",
    "    # Phase statistics\n",
    "    features = {\n",
    "        # Direct phase stats\n",
    "        'phase_mean': np.mean(phase),\n",
    "        'phase_std': np.std(phase),\n",
    "        'phase_var': np.var(phase),\n",
    "        \n",
    "        # Phase derivative (instantaneous frequency) stats\n",
    "        'phase_diff_mean': np.mean(phase_diff),\n",
    "        'phase_diff_std': np.std(phase_diff),\n",
    "        'phase_diff_max': np.max(np.abs(phase_diff)),\n",
    "        \n",
    "        # Phase jitter (variance of inst. freq)\n",
    "        'phase_jitter': np.var(phase_diff),\n",
    "        \n",
    "        # Non-linear phase component\n",
    "        'phase_nonlinear': np.std(phase_unwrap - np.polyval(\n",
    "            np.polyfit(np.arange(len(phase_unwrap)), phase_unwrap, 1), \n",
    "            np.arange(len(phase_unwrap)))),\n",
    "        \n",
    "        # Phase histogram entropy\n",
    "        'phase_entropy': entropy(np.histogram(phase, bins=32, density=True)[0] + 1e-10),\n",
    "        \n",
    "        # Circular statistics\n",
    "        'phase_circular_mean': np.abs(np.mean(np.exp(1j * phase))),\n",
    "        'phase_circular_var': 1 - np.abs(np.mean(np.exp(1j * phase))),\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute phase features\n",
    "print(\"Computing phase features for training data...\")\n",
    "train_phase = [compute_phase_features(s) for s in train_signals_filt]\n",
    "train_phase_df = pd.DataFrame(train_phase)\n",
    "train_phase_df['class'] = train_labels_filt\n",
    "train_phase_df['snr'] = train_snr_filt\n",
    "\n",
    "print(\"Computing phase features for test data...\")\n",
    "test_phase = [compute_phase_features(s) for s in test_signals]\n",
    "test_phase_df = pd.DataFrame(test_phase)\n",
    "test_phase_df['class'] = test_labels\n",
    "test_phase_df['snr'] = test_snr\n",
    "test_phase_df['is_anomaly'] = test_binary\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase feature distribution by class\n",
    "print(\"\\nMean Phase Features by Class (Test Data):\")\n",
    "print(\"=\"*80)\n",
    "phase_cols = ['phase_std', 'phase_jitter', 'phase_entropy', 'phase_circular_var', 'phase_diff_std']\n",
    "display_df = test_phase_df.groupby('class')[phase_cols].mean().round(4)\n",
    "display_df['is_anomaly'] = ['No']*6 + ['YES']*3\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize phase features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "cols_to_plot = ['phase_std', 'phase_jitter', 'phase_entropy', 'phase_circular_var', 'phase_diff_std', 'phase_nonlinear']\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    ax = axes.flatten()[i]\n",
    "    known = test_phase_df[test_phase_df['is_anomaly'] == 0][col]\n",
    "    anomaly = test_phase_df[test_phase_df['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known (0-5)', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly (6-8)', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{col}')\n",
    "\n",
    "plt.suptitle('Phase Features: Known vs Anomaly', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_phase_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Entropy-Based Features\n",
    "\n",
    "Different entropy measures capture signal complexity:\n",
    "- Spectral entropy: Flatness of power spectrum\n",
    "- Sample entropy: Signal regularity/predictability\n",
    "- Permutation entropy: Time-series complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_features(signal):\n",
    "    \"\"\"Compute various entropy measures.\"\"\"\n",
    "    x = compute_complex_signal(signal)\n",
    "    amplitude = np.abs(x)\n",
    "    \n",
    "    # Spectral entropy (normalized)\n",
    "    spectrum = np.abs(fft(x))**2\n",
    "    spectrum_norm = spectrum / (np.sum(spectrum) + 1e-10)\n",
    "    spectral_ent = entropy(spectrum_norm + 1e-10)\n",
    "    \n",
    "    # Amplitude histogram entropy\n",
    "    amp_hist, _ = np.histogram(amplitude, bins=64, density=True)\n",
    "    amp_entropy = entropy(amp_hist + 1e-10)\n",
    "    \n",
    "    # I and Q channel entropies\n",
    "    i_hist, _ = np.histogram(signal[:, 0], bins=64, density=True)\n",
    "    q_hist, _ = np.histogram(signal[:, 1], bins=64, density=True)\n",
    "    i_entropy = entropy(i_hist + 1e-10)\n",
    "    q_entropy = entropy(q_hist + 1e-10)\n",
    "    \n",
    "    # Approximate entropy (simplified)\n",
    "    def approx_entropy(data, m=2, r_mult=0.2):\n",
    "        \"\"\"Simplified approximate entropy.\"\"\"\n",
    "        n = len(data)\n",
    "        r = r_mult * np.std(data)\n",
    "        \n",
    "        # Subsample for speed\n",
    "        step = max(1, n // 500)\n",
    "        data_sub = data[::step]\n",
    "        n_sub = len(data_sub)\n",
    "        \n",
    "        def count_similar(m_val):\n",
    "            count = 0\n",
    "            templates = np.array([data_sub[i:i+m_val] for i in range(n_sub - m_val)])\n",
    "            for i in range(len(templates)):\n",
    "                dists = np.max(np.abs(templates - templates[i]), axis=1)\n",
    "                count += np.sum(dists <= r) - 1  # Exclude self\n",
    "            return count / (len(templates) * (len(templates) - 1) + 1e-10)\n",
    "        \n",
    "        c_m = count_similar(m)\n",
    "        c_m1 = count_similar(m + 1)\n",
    "        \n",
    "        return -np.log((c_m1 + 1e-10) / (c_m + 1e-10))\n",
    "    \n",
    "    # Compute on amplitude (simplified for speed)\n",
    "    approx_ent = approx_entropy(amplitude[:512])\n",
    "    \n",
    "    # Spectral flatness (Wiener entropy)\n",
    "    spectrum_pos = spectrum[:len(spectrum)//2]\n",
    "    geometric_mean = np.exp(np.mean(np.log(spectrum_pos + 1e-10)))\n",
    "    arithmetic_mean = np.mean(spectrum_pos)\n",
    "    spectral_flatness = geometric_mean / (arithmetic_mean + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        'spectral_entropy': spectral_ent,\n",
    "        'amplitude_entropy': amp_entropy,\n",
    "        'i_entropy': i_entropy,\n",
    "        'q_entropy': q_entropy,\n",
    "        'approx_entropy': approx_ent,\n",
    "        'spectral_flatness': spectral_flatness,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute entropy features\n",
    "print(\"Computing entropy features for training data...\")\n",
    "train_entropy = [compute_entropy_features(s) for s in train_signals_filt]\n",
    "train_entropy_df = pd.DataFrame(train_entropy)\n",
    "train_entropy_df['class'] = train_labels_filt\n",
    "train_entropy_df['snr'] = train_snr_filt\n",
    "\n",
    "print(\"Computing entropy features for test data...\")\n",
    "test_entropy = [compute_entropy_features(s) for s in test_signals]\n",
    "test_entropy_df = pd.DataFrame(test_entropy)\n",
    "test_entropy_df['class'] = test_labels\n",
    "test_entropy_df['snr'] = test_snr\n",
    "test_entropy_df['is_anomaly'] = test_binary\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy by class\n",
    "print(\"\\nMean Entropy Features by Class (Test Data):\")\n",
    "print(\"=\"*80)\n",
    "entropy_cols = ['spectral_entropy', 'amplitude_entropy', 'approx_entropy', 'spectral_flatness']\n",
    "display_df = test_entropy_df.groupby('class')[entropy_cols].mean().round(4)\n",
    "display_df['is_anomaly'] = ['No']*6 + ['YES']*3\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "entropy_plot_cols = ['spectral_entropy', 'amplitude_entropy', 'i_entropy', 'q_entropy', 'approx_entropy', 'spectral_flatness']\n",
    "for i, col in enumerate(entropy_plot_cols):\n",
    "    ax = axes.flatten()[i]\n",
    "    known = test_entropy_df[test_entropy_df['is_anomaly'] == 0][col]\n",
    "    anomaly = test_entropy_df[test_entropy_df['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known (0-5)', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly (6-8)', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{col}')\n",
    "\n",
    "plt.suptitle('Entropy Features: Known vs Anomaly', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_entropy_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Constellation Geometry Features\n",
    "\n",
    "Analyze the IQ constellation structure:\n",
    "- Number of clusters (estimated via silhouette)\n",
    "- Cluster compactness\n",
    "- Radial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_constellation_features(signal):\n",
    "    \"\"\"Analyze IQ constellation geometry.\"\"\"\n",
    "    iq = signal.copy()  # (2048, 2)\n",
    "    \n",
    "    # Normalize to unit power\n",
    "    power = np.mean(iq[:, 0]**2 + iq[:, 1]**2)\n",
    "    iq = iq / (np.sqrt(power) + 1e-10)\n",
    "    \n",
    "    # Radial (amplitude) distribution\n",
    "    radii = np.sqrt(iq[:, 0]**2 + iq[:, 1]**2)\n",
    "    \n",
    "    # Angle distribution\n",
    "    angles = np.arctan2(iq[:, 1], iq[:, 0])\n",
    "    \n",
    "    # Cluster analysis (try k=4, common for QAM/PSK)\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Subsample for speed\n",
    "    iq_sub = iq[::4]\n",
    "    \n",
    "    # Try different k and get inertias\n",
    "    inertias = []\n",
    "    for k in [2, 4, 8]:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=3, random_state=42)\n",
    "        kmeans.fit(iq_sub)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Best k=4 clustering metrics\n",
    "    kmeans_4 = KMeans(n_clusters=4, n_init=5, random_state=42)\n",
    "    labels_4 = kmeans_4.fit_predict(iq_sub)\n",
    "    \n",
    "    # Within-cluster variance\n",
    "    cluster_variances = []\n",
    "    for k in range(4):\n",
    "        cluster_points = iq_sub[labels_4 == k]\n",
    "        if len(cluster_points) > 1:\n",
    "            cluster_variances.append(np.var(cluster_points))\n",
    "    mean_cluster_var = np.mean(cluster_variances) if cluster_variances else 0\n",
    "    \n",
    "    # Centroid distances (how spread are clusters)\n",
    "    centroids = kmeans_4.cluster_centers_\n",
    "    centroid_dists = cdist(centroids, centroids)\n",
    "    mean_centroid_dist = np.mean(centroid_dists[np.triu_indices(4, k=1)])\n",
    "    \n",
    "    # Radial statistics\n",
    "    features = {\n",
    "        'radius_mean': np.mean(radii),\n",
    "        'radius_std': np.std(radii),\n",
    "        'radius_skew': skew(radii),\n",
    "        'radius_kurtosis': kurtosis(radii),\n",
    "        \n",
    "        # Angle uniformity (high = uniform, low = clustered)\n",
    "        'angle_uniformity': np.abs(np.mean(np.exp(1j * angles))),\n",
    "        'angle_std': np.std(angles),\n",
    "        \n",
    "        # Cluster metrics\n",
    "        'cluster_variance': mean_cluster_var,\n",
    "        'centroid_spread': mean_centroid_dist,\n",
    "        'inertia_k2': inertias[0],\n",
    "        'inertia_k4': inertias[1],\n",
    "        'inertia_k8': inertias[2],\n",
    "        'inertia_ratio_4_2': inertias[1] / (inertias[0] + 1e-10),\n",
    "        'inertia_ratio_8_4': inertias[2] / (inertias[1] + 1e-10),\n",
    "        \n",
    "        # Quadrant distribution\n",
    "        'q1_frac': np.mean((iq[:, 0] > 0) & (iq[:, 1] > 0)),\n",
    "        'q2_frac': np.mean((iq[:, 0] < 0) & (iq[:, 1] > 0)),\n",
    "        'q3_frac': np.mean((iq[:, 0] < 0) & (iq[:, 1] < 0)),\n",
    "        'q4_frac': np.mean((iq[:, 0] > 0) & (iq[:, 1] < 0)),\n",
    "        'quadrant_balance': np.std([np.mean((iq[:, 0] > 0) & (iq[:, 1] > 0)),\n",
    "                                    np.mean((iq[:, 0] < 0) & (iq[:, 1] > 0)),\n",
    "                                    np.mean((iq[:, 0] < 0) & (iq[:, 1] < 0)),\n",
    "                                    np.mean((iq[:, 0] > 0) & (iq[:, 1] < 0))]),\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute constellation features\n",
    "print(\"Computing constellation features for training data...\")\n",
    "train_const = [compute_constellation_features(s) for s in train_signals_filt]\n",
    "train_const_df = pd.DataFrame(train_const)\n",
    "train_const_df['class'] = train_labels_filt\n",
    "train_const_df['snr'] = train_snr_filt\n",
    "\n",
    "print(\"Computing constellation features for test data...\")\n",
    "test_const = [compute_constellation_features(s) for s in test_signals]\n",
    "test_const_df = pd.DataFrame(test_const)\n",
    "test_const_df['class'] = test_labels\n",
    "test_const_df['snr'] = test_snr\n",
    "test_const_df['is_anomaly'] = test_binary\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constellation features by class\n",
    "print(\"\\nMean Constellation Features by Class (Test Data):\")\n",
    "print(\"=\"*80)\n",
    "const_cols = ['radius_std', 'angle_uniformity', 'cluster_variance', 'centroid_spread', 'quadrant_balance']\n",
    "display_df = test_const_df.groupby('class')[const_cols].mean().round(4)\n",
    "display_df['is_anomaly'] = ['No']*6 + ['YES']*3\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize constellation features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "const_plot_cols = ['radius_std', 'radius_kurtosis', 'angle_uniformity', 'cluster_variance', 'centroid_spread', 'quadrant_balance']\n",
    "for i, col in enumerate(const_plot_cols):\n",
    "    ax = axes.flatten()[i]\n",
    "    known = test_const_df[test_const_df['is_anomaly'] == 0][col]\n",
    "    anomaly = test_const_df[test_const_df['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known (0-5)', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly (6-8)', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{col}')\n",
    "\n",
    "plt.suptitle('Constellation Geometry Features: Known vs Anomaly', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_constellation_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Spectral Features (Advanced)\n",
    "\n",
    "More detailed spectral analysis:\n",
    "- Spectral moments\n",
    "- Bandwidth measures\n",
    "- Peak analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_features(signal):\n",
    "    \"\"\"Compute advanced spectral features.\"\"\"\n",
    "    x = compute_complex_signal(signal)\n",
    "    n = len(x)\n",
    "    \n",
    "    # FFT\n",
    "    spectrum = np.abs(fftshift(fft(x)))**2\n",
    "    freqs = fftshift(fftfreq(n))\n",
    "    \n",
    "    # Normalize spectrum\n",
    "    spectrum_norm = spectrum / (np.sum(spectrum) + 1e-10)\n",
    "    \n",
    "    # Spectral moments\n",
    "    spectral_centroid = np.sum(freqs * spectrum_norm)\n",
    "    spectral_spread = np.sqrt(np.sum((freqs - spectral_centroid)**2 * spectrum_norm))\n",
    "    spectral_skewness = np.sum((freqs - spectral_centroid)**3 * spectrum_norm) / (spectral_spread**3 + 1e-10)\n",
    "    spectral_kurtosis = np.sum((freqs - spectral_centroid)**4 * spectrum_norm) / (spectral_spread**4 + 1e-10)\n",
    "    \n",
    "    # Bandwidth (3dB, 10dB, 20dB)\n",
    "    spectrum_db = 10 * np.log10(spectrum + 1e-10)\n",
    "    max_db = np.max(spectrum_db)\n",
    "    \n",
    "    bw_3db = np.sum(spectrum_db > (max_db - 3)) / n\n",
    "    bw_10db = np.sum(spectrum_db > (max_db - 10)) / n\n",
    "    bw_20db = np.sum(spectrum_db > (max_db - 20)) / n\n",
    "    \n",
    "    # Peak analysis\n",
    "    peaks, properties = scipy_signal.find_peaks(spectrum, height=np.max(spectrum)*0.1, distance=10)\n",
    "    n_peaks = len(peaks)\n",
    "    \n",
    "    # Peak to average ratio\n",
    "    papr = np.max(spectrum) / (np.mean(spectrum) + 1e-10)\n",
    "    \n",
    "    # Roll-off frequency (95% energy)\n",
    "    cumsum = np.cumsum(spectrum_norm)\n",
    "    rolloff_idx = np.argmax(cumsum >= 0.95)\n",
    "    rolloff_freq = freqs[rolloff_idx]\n",
    "    \n",
    "    return {\n",
    "        'spectral_centroid': spectral_centroid,\n",
    "        'spectral_spread': spectral_spread,\n",
    "        'spectral_skewness': spectral_skewness,\n",
    "        'spectral_kurtosis': spectral_kurtosis,\n",
    "        'bandwidth_3db': bw_3db,\n",
    "        'bandwidth_10db': bw_10db,\n",
    "        'bandwidth_20db': bw_20db,\n",
    "        'n_spectral_peaks': n_peaks,\n",
    "        'papr': papr,\n",
    "        'rolloff_freq': rolloff_freq,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral features\n",
    "print(\"Computing spectral features for training data...\")\n",
    "train_spectral = [compute_spectral_features(s) for s in train_signals_filt]\n",
    "train_spectral_df = pd.DataFrame(train_spectral)\n",
    "train_spectral_df['class'] = train_labels_filt\n",
    "train_spectral_df['snr'] = train_snr_filt\n",
    "\n",
    "print(\"Computing spectral features for test data...\")\n",
    "test_spectral = [compute_spectral_features(s) for s in test_signals]\n",
    "test_spectral_df = pd.DataFrame(test_spectral)\n",
    "test_spectral_df['class'] = test_labels\n",
    "test_spectral_df['snr'] = test_snr\n",
    "test_spectral_df['is_anomaly'] = test_binary\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral features by class\n",
    "print(\"\\nMean Spectral Features by Class (Test Data):\")\n",
    "print(\"=\"*80)\n",
    "spectral_cols = ['spectral_spread', 'spectral_kurtosis', 'bandwidth_3db', 'n_spectral_peaks', 'papr']\n",
    "display_df = test_spectral_df.groupby('class')[spectral_cols].mean().round(4)\n",
    "display_df['is_anomaly'] = ['No']*6 + ['YES']*3\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectral features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "spectral_plot_cols = ['spectral_spread', 'spectral_kurtosis', 'bandwidth_3db', 'bandwidth_10db', 'papr', 'n_spectral_peaks']\n",
    "for i, col in enumerate(spectral_plot_cols):\n",
    "    ax = axes.flatten()[i]\n",
    "    known = test_spectral_df[test_spectral_df['is_anomaly'] == 0][col]\n",
    "    anomaly = test_spectral_df[test_spectral_df['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known (0-5)', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly (6-8)', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{col}')\n",
    "\n",
    "plt.suptitle('Spectral Features: Known vs Anomaly', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_spectral_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Cyclostationary Features\n",
    "\n",
    "Radio signals exhibit cyclostationary properties (periodic statistics). The cyclic autocorrelation can reveal symbol rate and modulation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cyclostationary_features(signal):\n",
    "    \"\"\"Compute cyclostationary features.\"\"\"\n",
    "    x = compute_complex_signal(signal)\n",
    "    n = len(x)\n",
    "    \n",
    "    # Autocorrelation of |x|^2 (reveals symbol rate)\n",
    "    power = np.abs(x)**2\n",
    "    power_centered = power - np.mean(power)\n",
    "    acf_power = np.correlate(power_centered, power_centered, mode='full')\n",
    "    acf_power = acf_power[n-1:] / (acf_power[n-1] + 1e-10)  # Normalize, keep positive lags\n",
    "    \n",
    "    # Find peaks in autocorrelation (cyclic frequencies)\n",
    "    peaks, _ = scipy_signal.find_peaks(acf_power[10:500], height=0.1, distance=5)\n",
    "    n_cyclic_peaks = len(peaks)\n",
    "    first_peak_lag = peaks[0] + 10 if len(peaks) > 0 else 0\n",
    "    \n",
    "    # Autocorrelation decay rate\n",
    "    acf_decay = np.mean(acf_power[100:200])\n",
    "    \n",
    "    # Second-order cyclic moment at alpha=0 (standard autocorrelation)\n",
    "    acf_x = np.correlate(x, x, mode='full')\n",
    "    acf_x = acf_x[n-1:] / (np.abs(acf_x[n-1]) + 1e-10)\n",
    "    acf_mag_decay = np.mean(np.abs(acf_x[100:200]))\n",
    "    \n",
    "    # Conjugate autocorrelation (important for BPSK vs QPSK)\n",
    "    conj_acf = np.correlate(x, np.conj(x), mode='full')\n",
    "    conj_acf = conj_acf[n-1:] / (np.abs(conj_acf[n-1]) + 1e-10)\n",
    "    conj_acf_ratio = np.abs(conj_acf[1]) / (np.abs(acf_x[1]) + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        'n_cyclic_peaks': n_cyclic_peaks,\n",
    "        'first_cyclic_lag': first_peak_lag,\n",
    "        'acf_power_decay': acf_decay,\n",
    "        'acf_signal_decay': acf_mag_decay,\n",
    "        'conj_acf_ratio': conj_acf_ratio,\n",
    "        'acf_power_50': acf_power[50],\n",
    "        'acf_power_100': acf_power[100],\n",
    "        'acf_power_200': acf_power[200],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cyclostationary features\n",
    "print(\"Computing cyclostationary features for training data...\")\n",
    "train_cyclo = [compute_cyclostationary_features(s) for s in train_signals_filt]\n",
    "train_cyclo_df = pd.DataFrame(train_cyclo)\n",
    "train_cyclo_df['class'] = train_labels_filt\n",
    "train_cyclo_df['snr'] = train_snr_filt\n",
    "\n",
    "print(\"Computing cyclostationary features for test data...\")\n",
    "test_cyclo = [compute_cyclostationary_features(s) for s in test_signals]\n",
    "test_cyclo_df = pd.DataFrame(test_cyclo)\n",
    "test_cyclo_df['class'] = test_labels\n",
    "test_cyclo_df['snr'] = test_snr\n",
    "test_cyclo_df['is_anomaly'] = test_binary\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclostationary features by class\n",
    "print(\"\\nMean Cyclostationary Features by Class (Test Data):\")\n",
    "print(\"=\"*80)\n",
    "cyclo_cols = ['n_cyclic_peaks', 'first_cyclic_lag', 'acf_power_decay', 'conj_acf_ratio']\n",
    "display_df = test_cyclo_df.groupby('class')[cyclo_cols].mean().round(4)\n",
    "display_df['is_anomaly'] = ['No']*6 + ['YES']*3\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cyclostationary features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "cyclo_plot_cols = ['n_cyclic_peaks', 'first_cyclic_lag', 'acf_power_decay', 'acf_signal_decay', 'conj_acf_ratio', 'acf_power_100']\n",
    "for i, col in enumerate(cyclo_plot_cols):\n",
    "    ax = axes.flatten()[i]\n",
    "    known = test_cyclo_df[test_cyclo_df['is_anomaly'] == 0][col]\n",
    "    anomaly = test_cyclo_df[test_cyclo_df['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known (0-5)', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly (6-8)', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{col}')\n",
    "\n",
    "plt.suptitle('Cyclostationary Features: Known vs Anomaly', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_cyclostationary_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Combine All Features and Evaluate Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "feature_dfs = [\n",
    "    test_cum_df.drop(['class', 'snr', 'is_anomaly'], axis=1),\n",
    "    test_phase_df.drop(['class', 'snr', 'is_anomaly'], axis=1),\n",
    "    test_entropy_df.drop(['class', 'snr', 'is_anomaly'], axis=1),\n",
    "    test_const_df.drop(['class', 'snr', 'is_anomaly'], axis=1),\n",
    "    test_spectral_df.drop(['class', 'snr', 'is_anomaly'], axis=1),\n",
    "    test_cyclo_df.drop(['class', 'snr', 'is_anomaly'], axis=1),\n",
    "]\n",
    "\n",
    "test_all_features = pd.concat(feature_dfs, axis=1)\n",
    "test_all_features['class'] = test_labels\n",
    "test_all_features['snr'] = test_snr\n",
    "test_all_features['is_anomaly'] = test_binary\n",
    "\n",
    "print(f\"Total features: {test_all_features.shape[1] - 3}\")\n",
    "print(f\"Feature names: {list(test_all_features.columns[:-3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for training\n",
    "train_feature_dfs = [\n",
    "    train_cum_df.drop(['class', 'snr'], axis=1),\n",
    "    train_phase_df.drop(['class', 'snr'], axis=1),\n",
    "    train_entropy_df.drop(['class', 'snr'], axis=1),\n",
    "    train_const_df.drop(['class', 'snr'], axis=1),\n",
    "    train_spectral_df.drop(['class', 'snr'], axis=1),\n",
    "    train_cyclo_df.drop(['class', 'snr'], axis=1),\n",
    "]\n",
    "\n",
    "train_all_features = pd.concat(train_feature_dfs, axis=1)\n",
    "train_all_features['class'] = train_labels_filt\n",
    "train_all_features['snr'] = train_snr_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance via separability (AUC for each feature)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "feature_cols = list(test_all_features.columns[:-3])\n",
    "y_true = test_all_features['is_anomaly'].values\n",
    "\n",
    "auc_scores = {}\n",
    "for col in feature_cols:\n",
    "    try:\n",
    "        # Handle potential NaN/Inf\n",
    "        values = test_all_features[col].values\n",
    "        valid_mask = np.isfinite(values)\n",
    "        if valid_mask.sum() > 100:\n",
    "            auc = roc_auc_score(y_true[valid_mask], values[valid_mask])\n",
    "            # Take max(auc, 1-auc) since direction doesn't matter\n",
    "            auc_scores[col] = max(auc, 1 - auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Sort by AUC\n",
    "sorted_features = sorted(auc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE RANKING BY SEPARABILITY (AUC)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Rank':<6}{'Feature':<30}{'AUC':>10}\")\n",
    "print(\"-\"*50)\n",
    "for i, (feat, auc) in enumerate(sorted_features[:25], 1):\n",
    "    print(f\"{i:<6}{feat:<30}{auc:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top discriminative features\n",
    "top_features = [f[0] for f in sorted_features[:12]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(top_features):\n",
    "    ax = axes[i]\n",
    "    known = test_all_features[test_all_features['is_anomaly'] == 0][col]\n",
    "    anomaly = test_all_features[test_all_features['is_anomaly'] == 1][col]\n",
    "    \n",
    "    ax.hist(known, bins=50, alpha=0.6, label='Known', color=KNOWN_COLOR, density=True)\n",
    "    ax.hist(anomaly, bins=50, alpha=0.6, label='Anomaly', color=ANOMALY_COLOR, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    auc = auc_scores.get(col, 0)\n",
    "    ax.set_title(f'{col}\\nAUC={auc:.3f}')\n",
    "\n",
    "plt.suptitle('Top 12 Discriminative Features', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_top_discriminative_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. PCA and t-SNE with Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use top features for visualization\n",
    "top_n = 20\n",
    "top_feat_names = [f[0] for f in sorted_features[:top_n]]\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_all_features[top_feat_names].values\n",
    "X_test = test_all_features[top_feat_names].values\n",
    "\n",
    "# Handle NaN/Inf\n",
    "X_train = np.nan_to_num(X_train, nan=0, posinf=0, neginf=0)\n",
    "X_test = np.nan_to_num(X_test, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "train_pca = pca.fit_transform(X_train_scaled)\n",
    "test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training PCA\n",
    "ax = axes[0]\n",
    "for class_idx in range(6):\n",
    "    mask = train_all_features['class'].values == class_idx\n",
    "    ax.scatter(train_pca[mask, 0], train_pca[mask, 1], \n",
    "               c=[CLASS_COLORS[class_idx]], label=f'Class {class_idx}', alpha=0.4, s=10)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('PCA - Training Data (Advanced Features)')\n",
    "ax.legend()\n",
    "\n",
    "# Test PCA with anomalies\n",
    "ax = axes[1]\n",
    "for class_idx in range(9):\n",
    "    mask = test_all_features['class'].values == class_idx\n",
    "    color = ANOMALY_COLOR if class_idx >= 6 else CLASS_COLORS[class_idx]\n",
    "    marker = 'x' if class_idx >= 6 else 'o'\n",
    "    size = 40 if class_idx >= 6 else 15\n",
    "    label = f'ANOMALY {class_idx}' if class_idx >= 6 else f'Class {class_idx}'\n",
    "    ax.scatter(test_pca[mask, 0], test_pca[mask, 1], \n",
    "               c=[color], label=label, alpha=0.6, marker=marker, s=size)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('PCA - Test Data (Known + Anomalies)')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_pca_advanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE on test data\n",
    "print(\"Running t-SNE on test data with advanced features...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "test_tsne = tsne.fit_transform(X_test_scaled)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# t-SNE colored by class\n",
    "ax = axes[0]\n",
    "for class_idx in range(9):\n",
    "    mask = test_all_features['class'].values == class_idx\n",
    "    color = ANOMALY_COLOR if class_idx >= 6 else CLASS_COLORS[class_idx]\n",
    "    marker = 'x' if class_idx >= 6 else 'o'\n",
    "    size = 50 if class_idx >= 6 else 20\n",
    "    label = f'ANOMALY {class_idx}' if class_idx >= 6 else f'Class {class_idx}'\n",
    "    ax.scatter(test_tsne[mask, 0], test_tsne[mask, 1], \n",
    "               c=[color], label=label, alpha=0.6, marker=marker, s=size)\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.set_title('t-SNE - Test Data (by Class)')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# t-SNE colored by known/anomaly\n",
    "ax = axes[1]\n",
    "known_mask = test_all_features['is_anomaly'].values == 0\n",
    "ax.scatter(test_tsne[known_mask, 0], test_tsne[known_mask, 1], \n",
    "           c=KNOWN_COLOR, label='Known (0-5)', alpha=0.5, s=20)\n",
    "ax.scatter(test_tsne[~known_mask, 0], test_tsne[~known_mask, 1], \n",
    "           c=ANOMALY_COLOR, label='Anomaly (6-8)', alpha=0.7, s=50, marker='x')\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.set_title('t-SNE - Known vs Anomaly')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_tsne_advanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Anomaly Detection with Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Prepare data\n",
    "y_test = test_all_features['is_anomaly'].values\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANOMALY DETECTION WITH ADVANCED FEATURES\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "print(\"\\n--- Isolation Forest ---\")\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=200)\n",
    "iso_forest.fit(X_train_scaled)\n",
    "y_pred_iso = (iso_forest.predict(X_test_scaled) == -1).astype(int)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_iso):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_iso):.3f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_iso):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM\n",
    "print(\"\\n--- One-Class SVM ---\")\n",
    "# Subsample training for speed\n",
    "train_sub_idx = np.random.choice(len(X_train_scaled), size=5000, replace=False)\n",
    "ocsvm = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')\n",
    "ocsvm.fit(X_train_scaled[train_sub_idx])\n",
    "y_pred_svm = (ocsvm.predict(X_test_scaled) == -1).astype(int)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_svm):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_svm):.3f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_svm):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Outlier Factor (novelty detection mode)\n",
    "print(\"\\n--- Local Outlier Factor ---\")\n",
    "lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\n",
    "lof.fit(X_train_scaled[train_sub_idx])\n",
    "y_pred_lof = (lof.predict(X_test_scaled) == -1).astype(int)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lof):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lof):.3f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lof):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-anomaly class detection rates\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-ANOMALY CLASS DETECTION RATES (Isolation Forest)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for anomaly_class in [6, 7, 8]:\n",
    "    mask = test_all_features['class'].values == anomaly_class\n",
    "    detection_rate = y_pred_iso[mask].mean()\n",
    "    print(f\"Anomaly Class {anomaly_class}: {detection_rate:.1%} detected ({y_pred_iso[mask].sum()}/{mask.sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Per-Class Feature Analysis (Why is Class 7 Hard?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison: Class 7 vs similar known classes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARING ANOMALY CLASS 7 WITH SIMILAR KNOWN CLASSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Focus on classes 3, 4, 5 (low-power group) and anomaly 7\n",
    "comparison_classes = [3, 4, 5, 7]\n",
    "comparison_features = top_feat_names[:10]\n",
    "\n",
    "print(\"\\nMean values for top features:\")\n",
    "display_df = test_all_features[test_all_features['class'].isin(comparison_classes)].groupby('class')[comparison_features].mean().round(4).T\n",
    "display_df['Class7_vs_345_diff'] = np.abs(display_df[7] - display_df[[3,4,5]].mean(axis=1))\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features that best separate class 7 from classes 3,4,5\n",
    "class_7_mask = test_all_features['class'].values == 7\n",
    "class_345_mask = np.isin(test_all_features['class'].values, [3, 4, 5])\n",
    "\n",
    "separability_7_vs_345 = {}\n",
    "for col in feature_cols:\n",
    "    try:\n",
    "        values = test_all_features[col].values\n",
    "        valid = np.isfinite(values)\n",
    "        if valid.sum() > 50:\n",
    "            y_binary = np.zeros(len(values))\n",
    "            y_binary[class_7_mask] = 1\n",
    "            mask = valid & (class_7_mask | class_345_mask)\n",
    "            if mask.sum() > 50 and y_binary[mask].sum() > 10:\n",
    "                auc = roc_auc_score(y_binary[mask], values[mask])\n",
    "                separability_7_vs_345[col] = max(auc, 1-auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "sorted_sep_7 = sorted(separability_7_vs_345.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURES THAT BEST SEPARATE CLASS 7 FROM CLASSES 3,4,5\")\n",
    "print(\"=\"*60)\n",
    "for i, (feat, auc) in enumerate(sorted_sep_7[:15], 1):\n",
    "    print(f\"{i:>3}. {feat:<30} AUC = {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize best separating features for class 7\n",
    "best_for_7 = [f[0] for f in sorted_sep_7[:6]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(best_for_7):\n",
    "    ax = axes[i]\n",
    "    for class_idx in [3, 4, 5, 7]:\n",
    "        mask = test_all_features['class'].values == class_idx\n",
    "        color = ANOMALY_COLOR if class_idx == 7 else CLASS_COLORS[class_idx]\n",
    "        label = f'ANOMALY 7' if class_idx == 7 else f'Class {class_idx}'\n",
    "        values = test_all_features[mask][col].values\n",
    "        values = values[np.isfinite(values)]\n",
    "        ax.hist(values, bins=30, alpha=0.5, label=label, color=color, density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.legend()\n",
    "    auc = separability_7_vs_345.get(col, 0)\n",
    "    ax.set_title(f'{col}\\nAUC(7 vs 3,4,5) = {auc:.3f}')\n",
    "\n",
    "plt.suptitle('Features Best Separating Anomaly 7 from Classes 3,4,5', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_class7_separability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Save All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive feature sets\n",
    "train_all_features.to_csv('train_advanced_features.csv', index=False)\n",
    "test_all_features.to_csv('test_advanced_features.csv', index=False)\n",
    "\n",
    "print(f\"Saved train_advanced_features.csv: {train_all_features.shape}\")\n",
    "print(f\"Saved test_advanced_features.csv: {test_all_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DEEP DIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. FEATURE CATEGORIES EXPLORED:\")\n",
    "print(\"   - Higher-order cumulants (9 features): Modulation-specific signatures\")\n",
    "print(\"   - Phase features (11 features): Phase jitter, entropy, circular stats\")\n",
    "print(\"   - Entropy features (6 features): Spectral, amplitude, approximate entropy\")\n",
    "print(\"   - Constellation geometry (17 features): Cluster analysis, radial stats\")\n",
    "print(\"   - Spectral features (10 features): Bandwidth, peaks, PAPR\")\n",
    "print(\"   - Cyclostationary features (8 features): Autocorrelation, cyclic peaks\")\n",
    "\n",
    "print(f\"\\n2. TOTAL FEATURES: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\n3. TOP 10 DISCRIMINATIVE FEATURES (by AUC):\")\n",
    "for i, (feat, auc) in enumerate(sorted_features[:10], 1):\n",
    "    print(f\"   {i:>2}. {feat:<30} AUC = {auc:.4f}\")\n",
    "\n",
    "print(\"\\n4. ANOMALY DETECTION RESULTS (with advanced features):\")\n",
    "print(f\"   - Isolation Forest:  P={precision_score(y_test, y_pred_iso):.1%}, R={recall_score(y_test, y_pred_iso):.1%}, F1={f1_score(y_test, y_pred_iso):.1%}\")\n",
    "print(f\"   - One-Class SVM:     P={precision_score(y_test, y_pred_svm):.1%}, R={recall_score(y_test, y_pred_svm):.1%}, F1={f1_score(y_test, y_pred_svm):.1%}\")\n",
    "print(f\"   - LOF:               P={precision_score(y_test, y_pred_lof):.1%}, R={recall_score(y_test, y_pred_lof):.1%}, F1={f1_score(y_test, y_pred_lof):.1%}\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS:\")\n",
    "print(\"   a) Higher-order cumulants (C40, C42) provide modulation-specific signatures\")\n",
    "print(\"   b) Constellation geometry reveals structural differences in IQ plane\")\n",
    "print(\"   c) Phase features capture modulation dynamics\")\n",
    "print(\"   d) Class 7 remains hardest - overlaps with classes 3,4,5 in many features\")\n",
    "print(\"   e) Deep learning can learn even better representations from raw signals\")\n",
    "\n",
    "print(\"\\n6. RECOMMENDATIONS FOR MODELING:\")\n",
    "print(\"   a) Use top discriminative features as additional inputs to deep model\")\n",
    "print(\"   b) Design network architecture that captures:\")\n",
    "print(\"      - Higher-order statistics (via higher-order pooling layers)\")\n",
    "print(\"      - Phase dynamics (via phase-aware convolutions)\")\n",
    "print(\"      - Multi-scale features (via wavelet/multi-resolution analysis)\")\n",
    "print(\"   c) Consider contrastive learning to maximize class separation\")\n",
    "print(\"   d) Focus on detecting Class 7 - may need specialized handling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
