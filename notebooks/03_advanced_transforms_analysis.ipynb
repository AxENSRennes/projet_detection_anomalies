{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Deep Dive: Advanced Signal Transforms for Anomaly Detection\n",
    "\n",
    "Building on the EDA and Deep Dive findings, this notebook explores **specialized transforms** that exploit the observed signal characteristics:\n",
    "\n",
    "## Key Insights to Exploit\n",
    "1. **Symbol timing/ACF** was highly discriminative (AUC=0.75) → Cyclostationary analysis\n",
    "2. **Higher-order cumulants** (C42) achieved AUC=0.79 → Time-varying cumulants, bispectrum\n",
    "3. **Amplitude kurtosis** achieved AUC=0.94 for Class 7 → Multi-scale kurtosis\n",
    "4. **Constellation geometry** matters → Dictionary learning on IQ patterns\n",
    "\n",
    "## Transforms to Explore\n",
    "- **Wavelet Scattering Transform**: Invariant multi-scale representations\n",
    "- **Continuous Wavelet Transform**: Time-frequency with optimal resolution\n",
    "- **Cyclic Spectral Analysis**: Exploit cyclostationary nature\n",
    "- **Time-Varying Cumulants**: Local higher-order statistics\n",
    "- **Dictionary Learning**: Learn modulation-specific atoms\n",
    "- **Hilbert-Huang Transform (EMD)**: For non-stationary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Signal processing\n",
    "from scipy import signal\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.fft import fft, fftfreq\n",
    "import pywt  # PyWavelets\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import DictionaryLearning, SparseCoder\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Data loading\n",
    "from data_utils import load_train_data, load_test_anomalies, filter_by_snr, create_binary_labels\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_signals, train_labels, train_snr = load_train_data()\n",
    "test_signals, test_labels, test_snr = load_test_anomalies()\n",
    "\n",
    "# Filter out SNR=0 from training (not present in test)\n",
    "train_signals_filtered, train_labels_filtered, train_snr_filtered = filter_by_snr(\n",
    "    train_signals, train_labels, train_snr, [10, 20, 30]\n",
    ")\n",
    "\n",
    "# Create binary labels for test\n",
    "test_binary = create_binary_labels(test_labels)\n",
    "\n",
    "print(f\"Train (filtered): {train_signals_filtered.shape[0]} samples\")\n",
    "print(f\"Test: {test_signals.shape[0]} samples ({test_binary.sum()} anomalies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Convert IQ to complex\n",
    "def iq_to_complex(signal):\n",
    "    \"\"\"Convert (N, 2) IQ signal to complex array.\"\"\"\n",
    "    return signal[:, 0] + 1j * signal[:, 1]\n",
    "\n",
    "# Helper: Compute AUC\n",
    "def compute_auc(features_train, features_test, labels_test):\n",
    "    \"\"\"Compute AUC using Isolation Forest trained on known samples.\"\"\"\n",
    "    # Get indices of known classes in test\n",
    "    known_mask = labels_test == 0\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(features_train)\n",
    "    X_test = scaler.transform(features_test)\n",
    "    \n",
    "    # Train Isolation Forest\n",
    "    clf = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "    clf.fit(X_train)\n",
    "    \n",
    "    # Score test samples (lower = more anomalous)\n",
    "    scores = -clf.score_samples(X_test)\n",
    "    \n",
    "    return roc_auc_score(labels_test, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Wavelet Scattering Transform\n",
    "\n",
    "The **Wavelet Scattering Transform** creates translation-invariant, deformation-stable representations. It's particularly good for capturing:\n",
    "- Multi-scale modulation patterns\n",
    "- Hierarchical signal structure\n",
    "\n",
    "This could capture the symbol-level patterns that made ACF discriminative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified wavelet scattering using PyWavelets\n",
    "# (Full scattering would use kymatio, but we'll implement a simplified version)\n",
    "\n",
    "def wavelet_scattering_features(signal_iq, wavelet='db4', levels=6):\n",
    "    \"\"\"\n",
    "    Compute simplified wavelet scattering features.\n",
    "    \n",
    "    Scattering transform: S[x] = |x * ψ_j| * φ\n",
    "    We compute energy at each scale and statistics of coefficients.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process I and Q channels\n",
    "    for channel in [0, 1]:\n",
    "        x = signal_iq[:, channel]\n",
    "        \n",
    "        # Multi-level wavelet decomposition\n",
    "        coeffs = pywt.wavedec(x, wavelet, level=levels)\n",
    "        \n",
    "        # First order scattering: |W_j x|\n",
    "        for j, c in enumerate(coeffs):\n",
    "            # Energy at each scale\n",
    "            features.append(np.mean(np.abs(c)**2))\n",
    "            # Mean absolute value (first moment)\n",
    "            features.append(np.mean(np.abs(c)))\n",
    "            # Kurtosis at each scale\n",
    "            features.append(kurtosis(c))\n",
    "            # Variance\n",
    "            features.append(np.var(c))\n",
    "            \n",
    "        # Second order scattering: ||W_j x| * W_k|\n",
    "        # Apply wavelet to modulus of first-order coefficients\n",
    "        for j, c in enumerate(coeffs[1:4]):  # Detail coefficients at first 3 scales\n",
    "            modulus = np.abs(c)\n",
    "            if len(modulus) >= 4:\n",
    "                coeffs2 = pywt.wavedec(modulus, wavelet, level=min(3, int(np.log2(len(modulus)))))\n",
    "                for c2 in coeffs2:\n",
    "                    features.append(np.mean(np.abs(c2)))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test on one signal\n",
    "test_feat = wavelet_scattering_features(train_signals[0])\n",
    "print(f\"Scattering features per signal: {len(test_feat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scattering features for all signals\n",
    "print(\"Computing wavelet scattering features...\")\n",
    "\n",
    "# Use filtered training data\n",
    "train_scatter = np.array([wavelet_scattering_features(s) for s in train_signals_filtered])\n",
    "test_scatter = np.array([wavelet_scattering_features(s) for s in test_signals])\n",
    "\n",
    "print(f\"Train scattering shape: {train_scatter.shape}\")\n",
    "print(f\"Test scattering shape: {test_scatter.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_scatter = compute_auc(train_scatter, test_scatter, test_binary)\n",
    "print(f\"\\nWavelet Scattering AUC: {auc_scatter:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scattering coefficients by class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "\n",
    "for class_id in range(9):\n",
    "    ax = axes[class_id // 3, class_id % 3]\n",
    "    \n",
    "    if class_id < 6:\n",
    "        # Training class\n",
    "        mask = train_labels_filtered == class_id\n",
    "        data = train_scatter[mask]\n",
    "        color = 'blue' if class_id < 3 else 'green'\n",
    "    else:\n",
    "        # Test anomaly class\n",
    "        mask = test_labels == class_id\n",
    "        data = test_scatter[mask]\n",
    "        color = 'red'\n",
    "    \n",
    "    # Plot mean scattering spectrum\n",
    "    mean_scatter = data.mean(axis=0)\n",
    "    std_scatter = data.std(axis=0)\n",
    "    \n",
    "    ax.plot(mean_scatter, color=color, linewidth=1.5)\n",
    "    ax.fill_between(range(len(mean_scatter)), \n",
    "                    mean_scatter - std_scatter, \n",
    "                    mean_scatter + std_scatter,\n",
    "                    alpha=0.3, color=color)\n",
    "    \n",
    "    title = f\"Class {class_id}\"\n",
    "    if class_id >= 6:\n",
    "        title += \" (ANOMALY)\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature index')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "plt.suptitle('Wavelet Scattering Features by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_wavelet_scattering.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Continuous Wavelet Transform (CWT) Analysis\n",
    "\n",
    "CWT provides better time-frequency resolution than STFT. Let's analyze which scales are most discriminative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwt_features(signal_iq, wavelet='morl', scales=np.arange(1, 64)):\n",
    "    \"\"\"\n",
    "    Compute features from Continuous Wavelet Transform.\n",
    "    Uses Morlet wavelet which is good for oscillatory signals.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process complex signal\n",
    "    x_complex = iq_to_complex(signal_iq)\n",
    "    x_envelope = np.abs(x_complex)\n",
    "    x_phase = np.unwrap(np.angle(x_complex))\n",
    "    \n",
    "    for x, name in [(x_envelope, 'env'), (np.diff(x_phase), 'freq')]:\n",
    "        # CWT\n",
    "        coeffs, _ = pywt.cwt(x, scales, wavelet)\n",
    "        \n",
    "        # Energy per scale\n",
    "        scale_energy = np.mean(np.abs(coeffs)**2, axis=1)\n",
    "        features.extend(scale_energy)\n",
    "        \n",
    "        # Scale with max energy\n",
    "        features.append(np.argmax(scale_energy))\n",
    "        \n",
    "        # Energy ratio: low scales vs high scales\n",
    "        mid = len(scales) // 2\n",
    "        features.append(scale_energy[:mid].sum() / (scale_energy[mid:].sum() + 1e-10))\n",
    "        \n",
    "        # Kurtosis per scale (top 5 energy scales)\n",
    "        top_scales = np.argsort(scale_energy)[-5:]\n",
    "        for s in top_scales:\n",
    "            features.append(kurtosis(np.abs(coeffs[s])))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test\n",
    "test_cwt = cwt_features(train_signals[0])\n",
    "print(f\"CWT features per signal: {len(test_cwt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CWT features (subsample for speed)\n",
    "print(\"Computing CWT features...\")\n",
    "\n",
    "# Subsample for faster computation\n",
    "n_train_sample = 3000\n",
    "train_idx = np.random.choice(len(train_signals_filtered), n_train_sample, replace=False)\n",
    "\n",
    "train_cwt = np.array([cwt_features(train_signals_filtered[i]) for i in train_idx])\n",
    "test_cwt = np.array([cwt_features(s) for s in test_signals])\n",
    "\n",
    "print(f\"Train CWT shape: {train_cwt.shape}\")\n",
    "print(f\"Test CWT shape: {test_cwt.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_cwt = compute_auc(train_cwt, test_cwt, test_binary)\n",
    "print(f\"\\nCWT Features AUC: {auc_cwt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CWT scalograms for each class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "scales = np.arange(1, 64)\n",
    "\n",
    "for class_id in range(9):\n",
    "    ax = axes[class_id // 3, class_id % 3]\n",
    "    \n",
    "    if class_id < 6:\n",
    "        mask = train_labels_filtered == class_id\n",
    "        sig = train_signals_filtered[mask][0]\n",
    "    else:\n",
    "        mask = test_labels == class_id\n",
    "        sig = test_signals[mask][0]\n",
    "    \n",
    "    # CWT on envelope\n",
    "    x_complex = iq_to_complex(sig)\n",
    "    x_envelope = np.abs(x_complex)\n",
    "    coeffs, _ = pywt.cwt(x_envelope[:512], scales, 'morl')  # First 512 samples\n",
    "    \n",
    "    im = ax.imshow(np.abs(coeffs), aspect='auto', cmap='viridis',\n",
    "                   extent=[0, 512, scales[-1], scales[0]])\n",
    "    \n",
    "    title = f\"Class {class_id}\"\n",
    "    if class_id >= 6:\n",
    "        title += \" (ANOMALY)\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Scale')\n",
    "\n",
    "plt.suptitle('CWT Scalograms (Envelope)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_cwt_scalograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cyclostationary Analysis: Cyclic Autocorrelation\n",
    "\n",
    "Radio signals are **cyclostationary** - their statistics vary periodically. The symbol rate creates cyclic features.\n",
    "\n",
    "Since ACF features were discriminative (AUC=0.75), let's do a deeper cyclostationary analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclostationary_features(signal_iq, max_lag=200):\n",
    "    \"\"\"\n",
    "    Compute cyclostationary features:\n",
    "    - Cyclic autocorrelation at various lags\n",
    "    - Peak detection in cyclic domain\n",
    "    - Cyclic spectral features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    x_complex = iq_to_complex(signal_iq)\n",
    "    x_envelope = np.abs(x_complex)\n",
    "    x_power = x_envelope ** 2\n",
    "    \n",
    "    # 1. Cyclic autocorrelation of envelope\n",
    "    acf_env = np.correlate(x_envelope - x_envelope.mean(), \n",
    "                           x_envelope - x_envelope.mean(), mode='full')\n",
    "    acf_env = acf_env[len(acf_env)//2:]  # Positive lags only\n",
    "    acf_env = acf_env / acf_env[0]  # Normalize\n",
    "    \n",
    "    # ACF at specific lags (found discriminative in deep dive)\n",
    "    for lag in [10, 20, 30, 40, 50, 75, 100, 150, 200]:\n",
    "        if lag < len(acf_env):\n",
    "            features.append(acf_env[lag])\n",
    "    \n",
    "    # 2. Cyclic autocorrelation of squared signal (for symbol timing)\n",
    "    acf_power = np.correlate(x_power - x_power.mean(),\n",
    "                             x_power - x_power.mean(), mode='full')\n",
    "    acf_power = acf_power[len(acf_power)//2:]\n",
    "    acf_power = acf_power / (acf_power[0] + 1e-10)\n",
    "    \n",
    "    for lag in [10, 20, 30, 40, 50, 75, 100, 150, 200]:\n",
    "        if lag < len(acf_power):\n",
    "            features.append(acf_power[lag])\n",
    "    \n",
    "    # 3. Peak analysis in ACF\n",
    "    acf_short = acf_env[:max_lag]\n",
    "    peaks, properties = signal.find_peaks(acf_short, height=0.1, distance=5)\n",
    "    \n",
    "    features.append(len(peaks))  # Number of peaks\n",
    "    if len(peaks) > 0:\n",
    "        features.append(peaks[0])  # First peak location (fundamental period)\n",
    "        features.append(properties['peak_heights'].mean())  # Mean peak height\n",
    "        features.append(properties['peak_heights'].std())  # Peak height variation\n",
    "    else:\n",
    "        features.extend([0, 0, 0])\n",
    "    \n",
    "    # 4. Spectral analysis of ACF (cyclic spectrum)\n",
    "    acf_spectrum = np.abs(fft(acf_env[:512])[:256])\n",
    "    features.append(np.argmax(acf_spectrum))  # Dominant cyclic frequency\n",
    "    features.append(acf_spectrum.max() / (acf_spectrum.mean() + 1e-10))  # Cyclic peakiness\n",
    "    \n",
    "    # 5. Energy in different cyclic frequency bands\n",
    "    for band in [(0, 10), (10, 30), (30, 60), (60, 100), (100, 256)]:\n",
    "        features.append(acf_spectrum[band[0]:band[1]].sum())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test\n",
    "test_cyclo = cyclostationary_features(train_signals[0])\n",
    "print(f\"Cyclostationary features per signal: {len(test_cyclo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cyclostationary features\n",
    "print(\"Computing cyclostationary features...\")\n",
    "\n",
    "train_cyclo = np.array([cyclostationary_features(s) for s in train_signals_filtered])\n",
    "test_cyclo = np.array([cyclostationary_features(s) for s in test_signals])\n",
    "\n",
    "print(f\"Train cyclo shape: {train_cyclo.shape}\")\n",
    "print(f\"Test cyclo shape: {test_cyclo.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_cyclo = compute_auc(train_cyclo, test_cyclo, test_binary)\n",
    "print(f\"\\nCyclostationary Features AUC: {auc_cyclo:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cyclic autocorrelation by class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "\n",
    "for class_id in range(9):\n",
    "    ax = axes[class_id // 3, class_id % 3]\n",
    "    \n",
    "    if class_id < 6:\n",
    "        mask = train_labels_filtered == class_id\n",
    "        signals_class = train_signals_filtered[mask][:50]  # 50 samples\n",
    "        color = 'blue' if class_id < 3 else 'green'\n",
    "    else:\n",
    "        mask = test_labels == class_id\n",
    "        signals_class = test_signals[mask][:50]\n",
    "        color = 'red'\n",
    "    \n",
    "    # Compute mean ACF\n",
    "    acfs = []\n",
    "    for sig in signals_class:\n",
    "        x_env = np.abs(iq_to_complex(sig))\n",
    "        acf = np.correlate(x_env - x_env.mean(), x_env - x_env.mean(), mode='full')\n",
    "        acf = acf[len(acf)//2:300]\n",
    "        acf = acf / acf[0]\n",
    "        acfs.append(acf)\n",
    "    \n",
    "    acfs = np.array(acfs)\n",
    "    mean_acf = acfs.mean(axis=0)\n",
    "    std_acf = acfs.std(axis=0)\n",
    "    \n",
    "    ax.plot(mean_acf, color=color, linewidth=1.5)\n",
    "    ax.fill_between(range(len(mean_acf)), mean_acf - std_acf, mean_acf + std_acf,\n",
    "                    alpha=0.3, color=color)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    title = f\"Class {class_id}\"\n",
    "    if class_id >= 6:\n",
    "        title += \" (ANOMALY)\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Lag')\n",
    "    ax.set_ylabel('ACF')\n",
    "    ax.set_xlim([0, 300])\n",
    "\n",
    "plt.suptitle('Cyclic Autocorrelation (Envelope) by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_cyclic_acf.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Time-Varying Higher-Order Statistics\n",
    "\n",
    "Since C42 was discriminative globally, let's compute **local cumulants** over time windows to capture time-varying modulation characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_varying_cumulants(signal_iq, window_size=256, hop=128):\n",
    "    \"\"\"\n",
    "    Compute time-varying higher-order cumulants.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    x_complex = iq_to_complex(signal_iq)\n",
    "    \n",
    "    c20_list = []  # Variance\n",
    "    c21_list = []  # Conjugate variance\n",
    "    c40_list = []  # Fourth-order\n",
    "    c42_list = []  # Fourth-order mixed\n",
    "    kurt_list = []\n",
    "    \n",
    "    # Sliding window analysis\n",
    "    for start in range(0, len(x_complex) - window_size, hop):\n",
    "        x = x_complex[start:start + window_size]\n",
    "        x = x - x.mean()  # Zero-center\n",
    "        \n",
    "        # Second-order cumulants\n",
    "        c20 = np.mean(x * x)  # E[x^2]\n",
    "        c21 = np.mean(np.abs(x)**2)  # E[|x|^2]\n",
    "        \n",
    "        # Fourth-order cumulants\n",
    "        c40 = np.mean(x**4) - 3 * c20**2  # Kurtosis-like\n",
    "        c42 = np.mean(np.abs(x)**4) - np.abs(c20)**2 - 2 * c21**2  # Mixed cumulant\n",
    "        \n",
    "        c20_list.append(np.abs(c20))\n",
    "        c21_list.append(c21)\n",
    "        c40_list.append(np.abs(c40))\n",
    "        c42_list.append(c42)\n",
    "        kurt_list.append(kurtosis(np.abs(x)))\n",
    "    \n",
    "    # Statistics of time-varying cumulants\n",
    "    for series, name in [(c20_list, 'c20'), (c21_list, 'c21'), \n",
    "                         (c40_list, 'c40'), (c42_list, 'c42'),\n",
    "                         (kurt_list, 'kurt')]:\n",
    "        series = np.array(series)\n",
    "        features.append(series.mean())\n",
    "        features.append(series.std())\n",
    "        features.append(series.min())\n",
    "        features.append(series.max())\n",
    "        features.append(np.percentile(series, 25))\n",
    "        features.append(np.percentile(series, 75))\n",
    "        # Variation coefficient\n",
    "        features.append(series.std() / (np.abs(series.mean()) + 1e-10))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test\n",
    "test_tvcum = time_varying_cumulants(train_signals[0])\n",
    "print(f\"Time-varying cumulant features per signal: {len(test_tvcum)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time-varying cumulant features\n",
    "print(\"Computing time-varying cumulant features...\")\n",
    "\n",
    "train_tvcum = np.array([time_varying_cumulants(s) for s in train_signals_filtered])\n",
    "test_tvcum = np.array([time_varying_cumulants(s) for s in test_signals])\n",
    "\n",
    "print(f\"Train TV-cumulant shape: {train_tvcum.shape}\")\n",
    "print(f\"Test TV-cumulant shape: {test_tvcum.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_tvcum = compute_auc(train_tvcum, test_tvcum, test_binary)\n",
    "print(f\"\\nTime-Varying Cumulants AUC: {auc_tvcum:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time-varying C42 by class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "\n",
    "window_size = 256\n",
    "hop = 128\n",
    "\n",
    "for class_id in range(9):\n",
    "    ax = axes[class_id // 3, class_id % 3]\n",
    "    \n",
    "    if class_id < 6:\n",
    "        mask = train_labels_filtered == class_id\n",
    "        signals_class = train_signals_filtered[mask][:20]\n",
    "        color = 'blue' if class_id < 3 else 'green'\n",
    "    else:\n",
    "        mask = test_labels == class_id\n",
    "        signals_class = test_signals[mask][:20]\n",
    "        color = 'red'\n",
    "    \n",
    "    # Compute time-varying C42 for each signal\n",
    "    for sig in signals_class:\n",
    "        x_complex = iq_to_complex(sig)\n",
    "        c42_series = []\n",
    "        \n",
    "        for start in range(0, len(x_complex) - window_size, hop):\n",
    "            x = x_complex[start:start + window_size]\n",
    "            x = x - x.mean()\n",
    "            c20 = np.mean(x * x)\n",
    "            c21 = np.mean(np.abs(x)**2)\n",
    "            c42 = np.mean(np.abs(x)**4) - np.abs(c20)**2 - 2 * c21**2\n",
    "            c42_series.append(c42)\n",
    "        \n",
    "        ax.plot(c42_series, color=color, alpha=0.3, linewidth=0.8)\n",
    "    \n",
    "    title = f\"Class {class_id}\"\n",
    "    if class_id >= 6:\n",
    "        title += \" (ANOMALY)\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Window index')\n",
    "    ax.set_ylabel('C42')\n",
    "\n",
    "plt.suptitle('Time-Varying C42 Cumulant by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_time_varying_c42.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Dictionary Learning on IQ Patterns\n",
    "\n",
    "Learn a dictionary of **modulation-specific atoms** from the training data. Anomalies should have different sparse representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_iq_patches(signal_iq, patch_size=64, n_patches=32):\n",
    "    \"\"\"\n",
    "    Extract IQ patches from signal for dictionary learning.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    n_samples = len(signal_iq)\n",
    "    \n",
    "    for _ in range(n_patches):\n",
    "        start = np.random.randint(0, n_samples - patch_size)\n",
    "        patch = signal_iq[start:start + patch_size].flatten()  # (patch_size*2,)\n",
    "        patches.append(patch)\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "# Extract patches from training data\n",
    "print(\"Extracting IQ patches from training data...\")\n",
    "n_train_dict = 2000  # Use subset for dictionary learning\n",
    "train_idx_dict = np.random.choice(len(train_signals_filtered), n_train_dict, replace=False)\n",
    "\n",
    "all_patches = []\n",
    "for i in train_idx_dict:\n",
    "    patches = extract_iq_patches(train_signals_filtered[i], patch_size=64, n_patches=16)\n",
    "    all_patches.append(patches)\n",
    "\n",
    "all_patches = np.vstack(all_patches)\n",
    "print(f\"Total patches for dictionary learning: {all_patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn dictionary\n",
    "print(\"Learning dictionary...\")\n",
    "\n",
    "n_components = 64\n",
    "dict_learner = DictionaryLearning(\n",
    "    n_components=n_components,\n",
    "    alpha=1.0,\n",
    "    max_iter=500,\n",
    "    fit_algorithm='lars',\n",
    "    transform_algorithm='lasso_lars',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dict_learner.fit(all_patches)\n",
    "dictionary = dict_learner.components_\n",
    "print(f\"Dictionary shape: {dictionary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned dictionary atoms\n",
    "fig, axes = plt.subplots(8, 8, figsize=(14, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    atom = dictionary[i].reshape(64, 2)\n",
    "    ax.plot(atom[:, 0], 'b-', alpha=0.8, label='I')\n",
    "    ax.plot(atom[:, 1], 'r-', alpha=0.8, label='Q')\n",
    "    ax.set_title(f'Atom {i}', fontsize=8)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Learned Dictionary Atoms (IQ Patterns)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_dictionary_atoms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_features(signal_iq, dictionary, patch_size=64, n_patches=32):\n",
    "    \"\"\"\n",
    "    Compute features from sparse coding with learned dictionary.\n",
    "    \"\"\"\n",
    "    # Extract patches\n",
    "    patches = extract_iq_patches(signal_iq, patch_size=patch_size, n_patches=n_patches)\n",
    "    \n",
    "    # Sparse coding\n",
    "    coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_lars', transform_alpha=0.5)\n",
    "    codes = coder.transform(patches)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Statistics of sparse codes\n",
    "    features.append(np.mean(np.abs(codes)))  # Mean activation\n",
    "    features.append(np.std(np.abs(codes)))  # Activation variability\n",
    "    features.append((np.abs(codes) > 0.01).mean())  # Sparsity\n",
    "    \n",
    "    # Per-atom activation statistics\n",
    "    atom_activations = np.mean(np.abs(codes), axis=0)\n",
    "    features.extend(atom_activations)  # How much each atom is used\n",
    "    \n",
    "    # Reconstruction error\n",
    "    reconstructed = codes @ dictionary\n",
    "    error = np.mean((patches - reconstructed)**2)\n",
    "    features.append(error)\n",
    "    \n",
    "    # Per-patch reconstruction error variability\n",
    "    patch_errors = np.mean((patches - reconstructed)**2, axis=1)\n",
    "    features.append(np.std(patch_errors))\n",
    "    features.append(np.max(patch_errors))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test\n",
    "test_dict = dictionary_features(train_signals[0], dictionary)\n",
    "print(f\"Dictionary features per signal: {len(test_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dictionary features\n",
    "print(\"Computing dictionary features...\")\n",
    "\n",
    "train_dict_feat = np.array([dictionary_features(s, dictionary) for s in train_signals_filtered])\n",
    "test_dict_feat = np.array([dictionary_features(s, dictionary) for s in test_signals])\n",
    "\n",
    "print(f\"Train dict features shape: {train_dict_feat.shape}\")\n",
    "print(f\"Test dict features shape: {test_dict_feat.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_dict = compute_auc(train_dict_feat, test_dict_feat, test_binary)\n",
    "print(f\"\\nDictionary Learning Features AUC: {auc_dict:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize atom usage by class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "\n",
    "for class_id in range(9):\n",
    "    ax = axes[class_id // 3, class_id % 3]\n",
    "    \n",
    "    if class_id < 6:\n",
    "        mask = train_labels_filtered == class_id\n",
    "        data = train_dict_feat[mask]\n",
    "        color = 'blue' if class_id < 3 else 'green'\n",
    "    else:\n",
    "        mask = test_labels == class_id\n",
    "        data = test_dict_feat[mask]\n",
    "        color = 'red'\n",
    "    \n",
    "    # Atom activations are features 3 to 3+64\n",
    "    atom_usage = data[:, 3:3+64].mean(axis=0)\n",
    "    \n",
    "    ax.bar(range(64), atom_usage, color=color, alpha=0.7)\n",
    "    \n",
    "    title = f\"Class {class_id}\"\n",
    "    if class_id >= 6:\n",
    "        title += \" (ANOMALY)\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Atom index')\n",
    "    ax.set_ylabel('Mean activation')\n",
    "\n",
    "plt.suptitle('Dictionary Atom Usage by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_atom_usage.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Multi-Scale Kurtosis Analysis\n",
    "\n",
    "Since amplitude kurtosis achieved AUC=0.94 for Class 7, let's compute kurtosis at multiple scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiscale_kurtosis(signal_iq, scales=[32, 64, 128, 256, 512, 1024]):\n",
    "    \"\"\"\n",
    "    Compute kurtosis at multiple scales using wavelet decomposition and windowing.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    x_complex = iq_to_complex(signal_iq)\n",
    "    x_envelope = np.abs(x_complex)\n",
    "    x_phase_diff = np.diff(np.unwrap(np.angle(x_complex)))\n",
    "    \n",
    "    for data, name in [(x_envelope, 'env'), (x_phase_diff, 'freq')]:\n",
    "        # Multi-scale windowed kurtosis\n",
    "        for scale in scales:\n",
    "            if scale < len(data):\n",
    "                n_windows = len(data) // scale\n",
    "                windowed = data[:n_windows * scale].reshape(n_windows, scale)\n",
    "                kurt_values = [kurtosis(w) for w in windowed]\n",
    "                \n",
    "                features.append(np.mean(kurt_values))\n",
    "                features.append(np.std(kurt_values))\n",
    "                features.append(np.min(kurt_values))\n",
    "                features.append(np.max(kurt_values))\n",
    "        \n",
    "        # Wavelet-based multi-scale\n",
    "        coeffs = pywt.wavedec(data, 'db4', level=6)\n",
    "        for c in coeffs:\n",
    "            features.append(kurtosis(c))\n",
    "            features.append(skew(c))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test\n",
    "test_msk = multiscale_kurtosis(train_signals[0])\n",
    "print(f\"Multi-scale kurtosis features per signal: {len(test_msk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute multi-scale kurtosis features\n",
    "print(\"Computing multi-scale kurtosis features...\")\n",
    "\n",
    "train_msk = np.array([multiscale_kurtosis(s) for s in train_signals_filtered])\n",
    "test_msk = np.array([multiscale_kurtosis(s) for s in test_signals])\n",
    "\n",
    "print(f\"Train MSK shape: {train_msk.shape}\")\n",
    "print(f\"Test MSK shape: {test_msk.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_msk = compute_auc(train_msk, test_msk, test_binary)\n",
    "print(f\"\\nMulti-Scale Kurtosis AUC: {auc_msk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Constellation Geometry: Advanced Analysis\n",
    "\n",
    "Deep dive into IQ constellation patterns using radial and angular histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constellation_geometry_advanced(signal_iq, n_bins=32):\n",
    "    \"\"\"\n",
    "    Advanced constellation geometry features:\n",
    "    - Radial histogram\n",
    "    - Angular histogram\n",
    "    - 2D histogram entropy\n",
    "    - Symmetry features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    I = signal_iq[:, 0]\n",
    "    Q = signal_iq[:, 1]\n",
    "    radius = np.sqrt(I**2 + Q**2)\n",
    "    angle = np.arctan2(Q, I)\n",
    "    \n",
    "    # 1. Radial histogram\n",
    "    r_hist, r_bins = np.histogram(radius, bins=n_bins, density=True)\n",
    "    features.extend(r_hist)  # Full histogram as features\n",
    "    \n",
    "    # Radial statistics\n",
    "    features.append(radius.mean())\n",
    "    features.append(radius.std())\n",
    "    features.append(kurtosis(radius))\n",
    "    features.append(np.percentile(radius, 90) - np.percentile(radius, 10))  # Range\n",
    "    \n",
    "    # 2. Angular histogram\n",
    "    a_hist, a_bins = np.histogram(angle, bins=n_bins, range=(-np.pi, np.pi), density=True)\n",
    "    features.extend(a_hist)\n",
    "    \n",
    "    # Angular statistics\n",
    "    features.append(np.std(angle))  # Angular spread\n",
    "    features.append(kurtosis(angle))\n",
    "    \n",
    "    # 3. 2D histogram entropy\n",
    "    hist_2d, _, _ = np.histogram2d(I, Q, bins=n_bins)\n",
    "    hist_2d = hist_2d / hist_2d.sum()\n",
    "    hist_2d = hist_2d[hist_2d > 0]  # Remove zeros for entropy\n",
    "    entropy_2d = -np.sum(hist_2d * np.log(hist_2d + 1e-10))\n",
    "    features.append(entropy_2d)\n",
    "    \n",
    "    # 4. Symmetry features\n",
    "    # Quadrant occupancy\n",
    "    q1 = np.sum((I > 0) & (Q > 0)) / len(I)\n",
    "    q2 = np.sum((I < 0) & (Q > 0)) / len(I)\n",
    "    q3 = np.sum((I < 0) & (Q < 0)) / len(I)\n",
    "    q4 = np.sum((I > 0) & (Q < 0)) / len(I)\n",
    "    features.extend([q1, q2, q3, q4])\n",
    "    \n",
    "    # Quadrant balance (0 = perfect balance)\n",
    "    features.append(np.std([q1, q2, q3, q4]))\n",
    "    \n",
    "    # I/Q independence (correlation)\n",
    "    features.append(np.abs(np.corrcoef(I, Q)[0, 1]))\n",
    "    \n",
    "    # 5. Peak detection in radial histogram\n",
    "    peaks, _ = signal.find_peaks(r_hist, height=0.5)\n",
    "    features.append(len(peaks))  # Number of distinct radius levels\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test\n",
    "test_cga = constellation_geometry_advanced(train_signals[0])\n",
    "print(f\"Constellation geometry features per signal: {len(test_cga)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute constellation geometry features\n",
    "print(\"Computing constellation geometry features...\")\n",
    "\n",
    "train_cga = np.array([constellation_geometry_advanced(s) for s in train_signals_filtered])\n",
    "test_cga = np.array([constellation_geometry_advanced(s) for s in test_signals])\n",
    "\n",
    "print(f\"Train CGA shape: {train_cga.shape}\")\n",
    "print(f\"Test CGA shape: {test_cga.shape}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc_cga = compute_auc(train_cga, test_cga, test_binary)\n",
    "print(f\"\\nConstellation Geometry AUC: {auc_cga:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize radial and angular histograms by class\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 10))\n",
    "\n",
    "for class_id in range(9):\n",
    "    row = class_id // 3\n",
    "    col = (class_id % 3) * 2\n",
    "    \n",
    "    if class_id < 6:\n",
    "        mask = train_labels_filtered == class_id\n",
    "        signals_class = train_signals_filtered[mask][:100]\n",
    "        color = 'blue' if class_id < 3 else 'green'\n",
    "    else:\n",
    "        mask = test_labels == class_id\n",
    "        signals_class = test_signals[mask][:100]\n",
    "        color = 'red'\n",
    "    \n",
    "    # Compute mean histograms\n",
    "    r_hists = []\n",
    "    a_hists = []\n",
    "    \n",
    "    for sig in signals_class:\n",
    "        I, Q = sig[:, 0], sig[:, 1]\n",
    "        radius = np.sqrt(I**2 + Q**2)\n",
    "        angle = np.arctan2(Q, I)\n",
    "        \n",
    "        r_hist, _ = np.histogram(radius, bins=32, range=(0, 2), density=True)\n",
    "        a_hist, _ = np.histogram(angle, bins=32, range=(-np.pi, np.pi), density=True)\n",
    "        \n",
    "        r_hists.append(r_hist)\n",
    "        a_hists.append(a_hist)\n",
    "    \n",
    "    r_mean = np.mean(r_hists, axis=0)\n",
    "    a_mean = np.mean(a_hists, axis=0)\n",
    "    \n",
    "    # Radial histogram\n",
    "    ax1 = axes[row, col]\n",
    "    ax1.bar(range(32), r_mean, color=color, alpha=0.7)\n",
    "    title = f\"Class {class_id}\"\n",
    "    if class_id >= 6:\n",
    "        title += \" (A)\"\n",
    "    ax1.set_title(f\"{title} - Radial\")\n",
    "    ax1.set_xlabel('Radius bin')\n",
    "    \n",
    "    # Angular histogram\n",
    "    ax2 = axes[row, col + 1]\n",
    "    ax2.bar(range(32), a_mean, color=color, alpha=0.7)\n",
    "    ax2.set_title(f\"{title} - Angular\")\n",
    "    ax2.set_xlabel('Angle bin')\n",
    "\n",
    "plt.suptitle('Constellation Radial and Angular Histograms by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_constellation_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Combined Feature Analysis\n",
    "\n",
    "Combine all features and analyze which transforms are most discriminative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of individual feature set AUCs\n",
    "results = {\n",
    "    'Wavelet Scattering': auc_scatter,\n",
    "    'CWT Features': auc_cwt,\n",
    "    'Cyclostationary': auc_cyclo,\n",
    "    'Time-Varying Cumulants': auc_tvcum,\n",
    "    'Dictionary Learning': auc_dict,\n",
    "    'Multi-Scale Kurtosis': auc_msk,\n",
    "    'Constellation Geometry': auc_cga,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL FEATURE SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, auc in sorted(results.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{name:30s} AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine best feature sets\n",
    "print(\"\\nCombining feature sets...\")\n",
    "\n",
    "# All features combined\n",
    "train_combined = np.hstack([\n",
    "    train_scatter,\n",
    "    train_cyclo,\n",
    "    train_tvcum,\n",
    "    train_dict_feat,\n",
    "    train_msk,\n",
    "    train_cga,\n",
    "])\n",
    "\n",
    "test_combined = np.hstack([\n",
    "    test_scatter,\n",
    "    test_cyclo,\n",
    "    test_tvcum,\n",
    "    test_dict_feat,\n",
    "    test_msk,\n",
    "    test_cga,\n",
    "])\n",
    "\n",
    "print(f\"Combined features shape: {train_combined.shape}\")\n",
    "\n",
    "# Compute AUC for combined\n",
    "auc_combined = compute_auc(train_combined, test_combined, test_binary)\n",
    "print(f\"\\nCOMBINED Features AUC: {auc_combined:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-anomaly class performance\n",
    "print(\"\\nPer-Anomaly Class Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train Isolation Forest on combined features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_combined)\n",
    "X_test = scaler.transform(test_combined)\n",
    "\n",
    "clf = IsolationForest(contamination=0.1, random_state=42, n_estimators=200)\n",
    "clf.fit(X_train)\n",
    "scores = -clf.score_samples(X_test)\n",
    "\n",
    "for anomaly_class in [6, 7, 8]:\n",
    "    # Binary labels: 1 for this anomaly class, 0 for known classes\n",
    "    mask = (test_labels == anomaly_class) | (test_labels < 6)\n",
    "    y_binary = (test_labels[mask] == anomaly_class).astype(int)\n",
    "    scores_subset = scores[mask]\n",
    "    \n",
    "    auc = roc_auc_score(y_binary, scores_subset)\n",
    "    print(f\"Anomaly Class {anomaly_class}: AUC = {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall analysis\n",
    "print(\"\\nPrecision-Recall Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(test_binary, scores)\n",
    "\n",
    "# Find optimal threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best F1: {f1_scores[best_idx]:.3f}\")\n",
    "print(f\"At best F1 - Precision: {precision[best_idx]:.3f}, Recall: {recall[best_idx]:.3f}\")\n",
    "\n",
    "# Also show at fixed recall levels\n",
    "for target_recall in [0.5, 0.6, 0.7, 0.8]:\n",
    "    idx = np.argmin(np.abs(recall - target_recall))\n",
    "    print(f\"At Recall={recall[idx]:.2f}: Precision={precision[idx]:.3f}, F1={f1_scores[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. AUC comparison\n",
    "ax = axes[0]\n",
    "names = list(results.keys()) + ['COMBINED']\n",
    "aucs = list(results.values()) + [auc_combined]\n",
    "colors = ['steelblue'] * len(results) + ['darkgreen']\n",
    "bars = ax.barh(names, aucs, color=colors)\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', label='Random')\n",
    "ax.set_xlabel('AUC')\n",
    "ax.set_title('AUC by Feature Set')\n",
    "ax.set_xlim([0.5, 1.0])\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    ax.text(auc + 0.01, bar.get_y() + bar.get_height()/2, f'{auc:.3f}', \n",
    "            va='center', fontsize=9)\n",
    "\n",
    "# 2. Precision-Recall curve\n",
    "ax = axes[1]\n",
    "ax.plot(recall, precision, 'b-', linewidth=2)\n",
    "ax.scatter(recall[best_idx], precision[best_idx], color='red', s=100, \n",
    "           zorder=5, label=f'Best F1={f1_scores[best_idx]:.3f}')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve (Combined Features)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Score distribution\n",
    "ax = axes[2]\n",
    "ax.hist(scores[test_binary == 0], bins=50, alpha=0.6, label='Known (0-5)', density=True)\n",
    "ax.hist(scores[test_binary == 1], bins=50, alpha=0.6, label='Anomaly (6-8)', density=True)\n",
    "ax.axvline(x=best_threshold, color='red', linestyle='--', label=f'Threshold={best_threshold:.3f}')\n",
    "ax.set_xlabel('Anomaly Score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Score Distribution')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_advanced_transforms_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Which specific features from each transform are most discriminative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute individual feature AUCs\n",
    "print(\"Computing per-feature AUCs...\")\n",
    "\n",
    "feature_aucs = []\n",
    "for i in range(train_combined.shape[1]):\n",
    "    feat_train = train_combined[:, i:i+1]\n",
    "    feat_test = test_combined[:, i:i+1]\n",
    "    \n",
    "    # Simple AUC based on feature value\n",
    "    try:\n",
    "        # Handle inf/nan\n",
    "        valid_mask = np.isfinite(feat_test.flatten())\n",
    "        if valid_mask.sum() > 100:\n",
    "            auc = roc_auc_score(test_binary[valid_mask], feat_test.flatten()[valid_mask])\n",
    "            # Take the max of AUC and 1-AUC (in case direction is reversed)\n",
    "            auc = max(auc, 1 - auc)\n",
    "        else:\n",
    "            auc = 0.5\n",
    "    except:\n",
    "        auc = 0.5\n",
    "    \n",
    "    feature_aucs.append(auc)\n",
    "\n",
    "feature_aucs = np.array(feature_aucs)\n",
    "print(f\"Computed AUCs for {len(feature_aucs)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 features\n",
    "top_indices = np.argsort(feature_aucs)[-20:][::-1]\n",
    "\n",
    "# Create feature names\n",
    "feature_names = []\n",
    "# Scattering\n",
    "for i in range(train_scatter.shape[1]):\n",
    "    feature_names.append(f\"scatter_{i}\")\n",
    "# Cyclostationary\n",
    "for i in range(train_cyclo.shape[1]):\n",
    "    feature_names.append(f\"cyclo_{i}\")\n",
    "# TV Cumulants\n",
    "for i in range(train_tvcum.shape[1]):\n",
    "    feature_names.append(f\"tvcum_{i}\")\n",
    "# Dictionary\n",
    "for i in range(train_dict_feat.shape[1]):\n",
    "    feature_names.append(f\"dict_{i}\")\n",
    "# Multi-scale kurtosis\n",
    "for i in range(train_msk.shape[1]):\n",
    "    feature_names.append(f\"msk_{i}\")\n",
    "# Constellation geometry\n",
    "for i in range(train_cga.shape[1]):\n",
    "    feature_names.append(f\"cga_{i}\")\n",
    "\n",
    "print(\"\\nTop 20 Most Discriminative Features:\")\n",
    "print(\"=\"*60)\n",
    "for rank, idx in enumerate(top_indices):\n",
    "    print(f\"{rank+1:2d}. {feature_names[idx]:20s} AUC: {feature_aucs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "fig, axes = plt.subplots(4, 5, figsize=(18, 14))\n",
    "\n",
    "for i, (ax, idx) in enumerate(zip(axes.flat, top_indices)):\n",
    "    # Get feature values by class\n",
    "    for class_id in range(9):\n",
    "        if class_id < 6:\n",
    "            mask = train_labels_filtered == class_id\n",
    "            data = train_combined[mask, idx]\n",
    "            color = 'blue' if class_id < 3 else 'green'\n",
    "        else:\n",
    "            mask = test_labels == class_id\n",
    "            data = test_combined[mask, idx]\n",
    "            color = 'red'\n",
    "        \n",
    "        # Filter out inf/nan\n",
    "        data = data[np.isfinite(data)]\n",
    "        if len(data) > 0:\n",
    "            parts = ax.violinplot([data], [class_id], widths=0.7, showmeans=True)\n",
    "            for pc in parts['bodies']:\n",
    "                pc.set_facecolor(color)\n",
    "                pc.set_alpha(0.5)\n",
    "    \n",
    "    ax.set_title(f\"{feature_names[idx]}\\nAUC={feature_aucs[idx]:.3f}\", fontsize=9)\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_xticks(range(9))\n",
    "\n",
    "plt.suptitle('Top 20 Most Discriminative Features by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_top_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Feature Files for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined features\n",
    "import pandas as pd\n",
    "\n",
    "# Training features\n",
    "train_df = pd.DataFrame(train_combined, columns=feature_names)\n",
    "train_df['label'] = train_labels_filtered\n",
    "train_df['snr'] = train_snr_filtered\n",
    "train_df.to_csv('train_advanced_features.csv', index=False)\n",
    "print(f\"Saved train_advanced_features.csv: {train_df.shape}\")\n",
    "\n",
    "# Test features\n",
    "test_df = pd.DataFrame(test_combined, columns=feature_names)\n",
    "test_df['label'] = test_labels\n",
    "test_df['snr'] = test_snr\n",
    "test_df['binary_label'] = test_binary\n",
    "test_df.to_csv('test_advanced_features.csv', index=False)\n",
    "print(f\"Saved test_advanced_features.csv: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Transform | AUC | Key Insight |\n",
    "|-----------|-----|-------------|\n",
    "| **Wavelet Scattering** | ? | Multi-scale modulation patterns |\n",
    "| **CWT** | ? | Time-frequency with optimal resolution |\n",
    "| **Cyclostationary** | ? | Symbol timing exploitation |\n",
    "| **Time-Varying Cumulants** | ? | Local higher-order statistics |\n",
    "| **Dictionary Learning** | ? | Modulation-specific atoms |\n",
    "| **Multi-Scale Kurtosis** | ? | Kurtosis at different scales |\n",
    "| **Constellation Geometry** | ? | IQ plane structure |\n",
    "| **COMBINED** | ? | All features together |\n",
    "\n",
    "### Recommendations for Deep Learning\n",
    "\n",
    "1. **Architecture Design**: Include layers that can learn:\n",
    "   - Multi-scale wavelet-like filters\n",
    "   - Higher-order pooling (for cumulants)\n",
    "   - Attention on IQ relationships\n",
    "\n",
    "2. **Input Representation**: Consider using:\n",
    "   - Raw IQ + scattering coefficients\n",
    "   - CWT scalograms as 2D input\n",
    "   - Cyclic spectral features\n",
    "\n",
    "3. **Auxiliary Tasks**: Train with:\n",
    "   - Classification head for classes 0-5\n",
    "   - Reconstruction head (autoencoder)\n",
    "   - Auxiliary predictions of kurtosis/cumulants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exer": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
